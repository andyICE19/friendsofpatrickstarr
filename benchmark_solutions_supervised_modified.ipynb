{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated metadata matching\n",
    "\n",
    "Life sciences (LS)/ Clinical research institutes (academic research institutes, pharma companies, hospitals, clinics etc.) across the world are producing large volumes of data from patients. This can range from clinical information such as diagnostic/prognostic data, omic data such as genetic/proteomic/epigenetic screens, pathological data such as MRI scans etc. One of the main objectives in LS research (both academic and industrial) is to gain actionable insights from these data sets, that goes beyond the diagnosis/prognosis of a (group of) patient(s) and provides a deeper understanding of the diseases, as well as shine lights on new therapeutic options. It is becoming apparent, that to gain actionable insights from LS data sets, we need data from a large number of patients. This is achievable, if we could merge datasets from various institutes, which is turning out to be hugely challenging task, simply because different institutes use different standards, units, nomenclature etc. to store data. <br><br>\n",
    "\n",
    "For instance, patient's age is a common clinical parameter recorded by almost all organisations. One institute can name the variable that records patients’ age as 'patient age', another can name the same variable as 'age', others can name it as 'age at diagnosis', 'days since birth', 'years since birth' etc. The values can also be in days, months, years etc. Therefore, to combine data from many institutes (and sometimes within same institutes), it's essential to understand that all of the above variables are recording the same thing, i.e. patient's age, also we need to make sure that the units (days, months, years) of measuring age are homogenised at the time of integration. <br><br>\n",
    "\n",
    "To assist in the above process the National Cancer Institute (NCI) created the concept of CDE (common data element). See https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search for more details. A big data dump of about 69000 CDE elements are provided in 'cde_database\\full_database' folder in XML format, if you want to further explore. They provide a standard format of representing Life Science's data. This gives us standard variable name, the permissible values , units etc. for each of these clinical parameters. Some research organizations are following this standard, but vast majority aren't. Additionally, there is huge amount of data produced until now which are not standardized using CDEs. <br><br>\n",
    "\n",
    "To be able to integrate data from various institutes, we need to be able to match the variable names in the clinical datasets to the corresponding CDE elements. Currently, there is a drive for developing ML/AI algorithms to achieve this.<br><br>\n",
    "\n",
    "\n",
    "The code below is an initial attempt in this direction. In summary, it tries to match the variables names (generally the column headers in a clinical data file) and values (the column values) of the clinical parameters in a dataset, to the long variable names, and permissible values of the CDE elements. The objective is to find the CDE elements that closely match the each clinical parameter name (i.e. the column header). To do the the following steps are performed: <br><br>\n",
    "\n",
    "1. Converted selected aspects (e.g. long_name, permissible_values etc.) of all CDE elements into numerical vectors using a word embedding model which itself was trained on these data.\n",
    "\n",
    "2. Coverted the clinical parameter names (headers) and values into numerical vector using the same word embedding model as above.\n",
    "\n",
    "3. The vectors from the clinical data can be matched to CDE vectors in a few different ways: <br>\n",
    "   (a) One way is to use unsupervised learning, fit a Nearest Neighbour model to the CDE vectors, and look for the nearest neighbors of each clinical parameter using this model.\n",
    "   (b) Another way is to use supervised learning: Create feature vectors for all possible pairs of clinical parameters and CDE elements, consider the true pairs as positive class (target =1) and the remaining pairs as negative class (target = 0). Train classifiers to this data and use the classifier to evaluate new clinical parameters.\n",
    "   \n",
    " \n",
    "See more details below.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install custom benchmark solutions libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cde_modelling_tools\n",
      "Installing collected packages: cde-data-modeller\n",
      "  Found existing installation: cde-data-modeller 0.0.1\n",
      "    Uninstalling cde-data-modeller-0.0.1:\n",
      "      Successfully uninstalled cde-data-modeller-0.0.1\n",
      "  Running setup.py install for cde-data-modeller ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed cde-data-modeller-0.0.1\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 21.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cde_modelling_tools/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import mlflow\n",
    "from cde_modelling.modelling import CDE_data_modeller as cdm\n",
    "from cde_modelling.modelling.create_models import Model\n",
    "from cde_modelling.modelling.cross_val import prep_ml, log_result_for_gridsearch, make_predictions\n",
    "from cde_modelling.parsing import TCGA_data_parser as tdp\n",
    "from cde_modelling.utils import Accuracy_calculations as ac\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "clinical_data_files_dir = 'training_data/'\n",
    "\n",
    "clinical_data_test_dir = 'test_data/'\n",
    "\n",
    "cde_database_file = 'cde_database/combined_small_dataset.json'\n",
    "\n",
    "parameter_file = 'params_supervised.json'\n",
    "\n",
    "model_dir = 'models/'\n",
    "\n",
    "#test_gold_standard = 'gold_standard/test_gs.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model parameters\n",
    "params = {}\n",
    "\n",
    "with open(parameter_file,'r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "#change window size here\n",
    "params['fasttext']['window'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fasttext model for the CDE database and index the individual CDE elements in the database \n",
    "\n",
    "Fasttext is a word embedding algorithm developed by FaceBook. Given a corpus, it creates a model that tries to predict if a pair of words appear in the same context. The model first converts the words to a numeric vector which are used as features for the above prediction. We are interested in the feature generation part, i.e. the part which converts words to numeric vectors. For more information on the FastText model see https://radimrehurek.com/gensim/models/fasttext.html.\n",
    "\n",
    "### FastText model training: \n",
    "\n",
    "To train a FastText model we first extracted the long_name and permissible_values of each CDE elements. These were then parseed and cleaned (lower cased, alphanumeric character only, splitted into bag of words). The preprocessed long names and permissible values of all CDE element was considered as the training corpus for the FastText model. The corpus was then used to train A FastText model. The parameters for the model are in the above json file. The trained model is then used to index the CDE elements (i.e. create numeric vectors representing each CDE). We created two sets of vectors for CDE elements, one for the long_names and the other for permissible values. We alo extracted the data_type information for each CDE elements. Below is an example. Let's assume that the following is a (oversimplified) CDE element .\n",
    "CDE_element: \n",
    "{\n",
    "'public_id': 1234\n",
    "'long_name': 'received radiotherapy'\n",
    "..............\n",
    "'permissible_values': ['yes','no']\n",
    "}.\n",
    "\n",
    "To index the above CDE, we performed the following:\n",
    "\n",
    "1. Vectorized the long_name entry (i.e. 'received radiotherapy') using the FastText model. To do that, we vectorized each word (i.e. 'received' and 'radiotherapy') of the long name entry separately. The vectors were then normalized by their L2 norms and averaged. Say for example, the long_name vector is [0.1, 0.345]. \n",
    "\n",
    "2. Vectorized the 'permissible_values' entry (i.e. 'yes', 'no') using the FastText model. To do that, we vectorized each word (i.e. 'received' and 'radiotherapy') in the permissible_values entry separately. The vectors were then normalized by their L2 norms and averaged.Say for example, the permissible vector is [0.981, 0.233]. \n",
    "\n",
    "3. We identified whether the permissible values are string or numbers. Note, that for the benchmark solution, we kept this simple. But for the hackathon, the participants can conder more grannular data type for example, string, binary, float, int long etc.\n",
    "\n",
    "\n",
    "Combination of the above is used to numerically represent (index) each CDE. The class CDE_data_modeller, in package cde_modelling_tools does the above. Pparticipants should explore using other entries in the CDE data fields to improve their chances of finding a match.\n",
    "\n",
    "The CDE_data_modeller class not only creates the word embedding models and index (vectorize) the CDE data elements, it can also save and load pretrained models and indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_data_modellers = cdm.CDE_data_modeller(cde_database_file, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "cde_data_modellers.create_model_and_cde_indexes()\n",
    "cde_data_modellers.save_model_and_indexes(model_dir+'window_'+str(params['fasttext']['window'])+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained FastText model and saved indexes for CDE elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CDE database... please wait\n",
      "Took 0.001874  minutes to load CDE database..\n"
     ]
    }
   ],
   "source": [
    "cde_data_modellers = cdm.CDE_data_modeller(cde_database_file, params)\n",
    "cde_data_modellers.load_model_and_cde_indexes(model_dir+'window_'+str(params['fasttext']['window'])+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and parse training data\n",
    "\n",
    "The training data are a set of clinical data files which records cinical information of patients, e.g. gender, age, disease_type, disease_sub_type, treatment received etc. It's in table format, where the rows represent patients and the columns represent colinical parameters. In case of the training data, the CDE data element corrsponding to each clinical parameter is provided. This information can be used to train machine learning algorithms to predict CDE elements for new clinical parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "  4%|▎         | 6/171 [00:00<00:03, 51.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing clinical metadata.. please wait..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:03<00:00, 46.82it/s]\n"
     ]
    }
   ],
   "source": [
    "tdpr = tdp.TCGA_data_processor(clinical_data_files_dir,True )\n",
    "tcga_data = tdpr.get_parsed_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser returns three types of information for each clinical parameter.\n",
    "\n",
    "1. The name of the parameter (e.g. age, gender, etc.)\n",
    "2. List of values for each parameters (except id columns, continuous variabales etc.)\n",
    "3. Data type of the values. For instance, data type of 'age' is 'number', data type of gender = 'string'. \n",
    "4. A dictionary containing clinical parameters and it's corresponding \n",
    "\n",
    "See the parsed data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data['gold_standard']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base tables for model training\n",
    "\n",
    "To create base tables I performed the following:\n",
    "\n",
    "1. Indexed (vectorized using the FastText model) the headers (clinical parameter names) and values of each clinical parameters parsed in the previous step.\n",
    "2. for each possible pair of clinical_parameter and CDE elements we calculate the following features <br>\n",
    "    (a) Difference between the embedding vectors of the CDE long_name and the clinical parameter name. <br>\n",
    "    (b) Difference between the embedding vectors of the CDE permissible values and the values associated with the clinical parameters in the training dataset. <br>\n",
    "    (c) A similarity measure (cosine similarity, correlation etc.) betwween the CDE long_name and clinical parameter name vectors <br>\n",
    "    (d) A similarity measure (cosine similarity, correlation etc.) betwween the CDE permissible_vaue and observed clinical parameter value vectors <br>\n",
    "    (e) Similaritied between the data type of the permissible and observed values of the CDE and the observed clinical oaraneters respectively <br>\n",
    "\n",
    "3. Note that, in the base table one data point is represented by a pair (clinical parameter and a CDE ). For example: If there are 800 cinical parameters in the training data and 5000 CDE elements in the CDE dataset, the the base table will have 500*8000 = 4million entries. Each entry will have the above features. The 'target' variable is defined as follows: <br>\n",
    "\n",
    "$\n",
    "target = 1, \\text{if the CDE element is manually matched to the clinical parameter} \\\\\n",
    "target = 0, \\text{otherwise}\n",
    "$\n",
    "\n",
    "In the above example, there are 800 clinical parameters, and if only 1 CDE elements is matched to each clinical parameter, the target variable can be equals to 1 in only 800 out of 4 million cases. Therefore the base table is extremely imbalanced. To counter this we need to undersample (or oversample) the abt. The create_abt function in CDE_data_modeller allows undersampling. The ratio of undersampling (number of cases target = 0 / number of cases target =1 ) can be adjusted using the params dictionary. The defalut value is 5 which means in the undersampled base tables, 16.67 % of cases have target =1 and 83.33% of cases have target = 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data['headers'] # tcga_data['values'] / tcga_data['value_type'] / tcga_data['gold_standard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data['value_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['features']['differences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [00:00<00:00, 14296.68it/s]\n",
      "  2%|▏         | 10/643 [00:00<00:07, 87.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start converting descriptors to vectors\n",
      "Took 0.000834 minutes to vectorize the dataset\n",
      "Start converting descriptors to vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643/643 [00:00<00:00, 711.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.015133 minutes to vectorize the dataset\n"
     ]
    }
   ],
   "source": [
    "abt = cde_data_modellers.create_abt(tcga_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_vec_0_x</th>\n",
       "      <th>feature_vec_1_x</th>\n",
       "      <th>feature_vec_2_x</th>\n",
       "      <th>feature_vec_3_x</th>\n",
       "      <th>feature_vec_4_x</th>\n",
       "      <th>feature_vec_5_x</th>\n",
       "      <th>feature_vec_6_x</th>\n",
       "      <th>feature_vec_7_x</th>\n",
       "      <th>feature_vec_8_x</th>\n",
       "      <th>feature_vec_9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_vec_44_y</th>\n",
       "      <th>feature_vec_45_y</th>\n",
       "      <th>feature_vec_46_y</th>\n",
       "      <th>feature_vec_47_y</th>\n",
       "      <th>feature_vec_48_y</th>\n",
       "      <th>feature_vec_49_y</th>\n",
       "      <th>header_metrics</th>\n",
       "      <th>value_metrics</th>\n",
       "      <th>metric</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20465</th>\n",
       "      <td>-0.105418</td>\n",
       "      <td>-0.061543</td>\n",
       "      <td>-0.117965</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>-0.059868</td>\n",
       "      <td>0.058310</td>\n",
       "      <td>0.031281</td>\n",
       "      <td>0.063111</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>0.094526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103615</td>\n",
       "      <td>0.028273</td>\n",
       "      <td>0.107822</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>0.075125</td>\n",
       "      <td>-0.009436</td>\n",
       "      <td>0.758509</td>\n",
       "      <td>0.772807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25276</th>\n",
       "      <td>0.003936</td>\n",
       "      <td>-0.035204</td>\n",
       "      <td>-0.065773</td>\n",
       "      <td>-0.055641</td>\n",
       "      <td>-0.078414</td>\n",
       "      <td>-0.074395</td>\n",
       "      <td>0.130359</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>-0.039931</td>\n",
       "      <td>-0.007264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038880</td>\n",
       "      <td>-0.110083</td>\n",
       "      <td>0.145582</td>\n",
       "      <td>-0.086970</td>\n",
       "      <td>0.032160</td>\n",
       "      <td>0.122551</td>\n",
       "      <td>0.783318</td>\n",
       "      <td>0.698063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>-0.058896</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>-0.082731</td>\n",
       "      <td>-0.092684</td>\n",
       "      <td>-0.099437</td>\n",
       "      <td>-0.158131</td>\n",
       "      <td>-0.031115</td>\n",
       "      <td>0.150547</td>\n",
       "      <td>-0.061018</td>\n",
       "      <td>0.112766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.046957</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.028463</td>\n",
       "      <td>-0.118065</td>\n",
       "      <td>0.707717</td>\n",
       "      <td>0.714520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35278</th>\n",
       "      <td>-0.111698</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>-0.077089</td>\n",
       "      <td>0.114752</td>\n",
       "      <td>-0.067237</td>\n",
       "      <td>0.124697</td>\n",
       "      <td>0.079938</td>\n",
       "      <td>0.144006</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>-0.062147</td>\n",
       "      <td>0.760834</td>\n",
       "      <td>0.750528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40138</th>\n",
       "      <td>0.032025</td>\n",
       "      <td>-0.030248</td>\n",
       "      <td>-0.076980</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>-0.066157</td>\n",
       "      <td>-0.012507</td>\n",
       "      <td>0.134601</td>\n",
       "      <td>0.070819</td>\n",
       "      <td>-0.071116</td>\n",
       "      <td>-0.025644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043698</td>\n",
       "      <td>-0.103946</td>\n",
       "      <td>0.145099</td>\n",
       "      <td>-0.091565</td>\n",
       "      <td>0.039552</td>\n",
       "      <td>0.126346</td>\n",
       "      <td>0.803004</td>\n",
       "      <td>0.691211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_vec_0_x  feature_vec_1_x  feature_vec_2_x  feature_vec_3_x  \\\n",
       "20465        -0.105418        -0.061543        -0.117965         0.020756   \n",
       "25276         0.003936        -0.035204        -0.065773        -0.055641   \n",
       "30103        -0.058896        -0.011462        -0.082731        -0.092684   \n",
       "35278        -0.111698         0.016187        -0.077089         0.114752   \n",
       "40138         0.032025        -0.030248        -0.076980         0.009730   \n",
       "\n",
       "       feature_vec_4_x  feature_vec_5_x  feature_vec_6_x  feature_vec_7_x  \\\n",
       "20465        -0.059868         0.058310         0.031281         0.063111   \n",
       "25276        -0.078414        -0.074395         0.130359         0.024135   \n",
       "30103        -0.099437        -0.158131        -0.031115         0.150547   \n",
       "35278        -0.067237         0.124697         0.079938         0.144006   \n",
       "40138        -0.066157        -0.012507         0.134601         0.070819   \n",
       "\n",
       "       feature_vec_8_x  feature_vec_9_x  ...  feature_vec_44_y  \\\n",
       "20465        -0.001067         0.094526  ...          0.103615   \n",
       "25276        -0.039931        -0.007264  ...         -0.038880   \n",
       "30103        -0.061018         0.112766  ...          0.007268   \n",
       "35278         0.003276        -0.004070  ...         -0.013448   \n",
       "40138        -0.071116        -0.025644  ...         -0.043698   \n",
       "\n",
       "       feature_vec_45_y  feature_vec_46_y  feature_vec_47_y  feature_vec_48_y  \\\n",
       "20465          0.028273          0.107822         -0.009273          0.075125   \n",
       "25276         -0.110083          0.145582         -0.086970          0.032160   \n",
       "30103         -0.046957         -0.000788          0.007931          0.028463   \n",
       "35278          0.030484          0.006091          0.056038          0.003965   \n",
       "40138         -0.103946          0.145099         -0.091565          0.039552   \n",
       "\n",
       "       feature_vec_49_y  header_metrics  value_metrics  metric  target  \n",
       "20465         -0.009436        0.758509       0.772807     0.0     1.0  \n",
       "25276          0.122551        0.783318       0.698063     0.0     1.0  \n",
       "30103         -0.118065        0.707717       0.714520     0.0     1.0  \n",
       "35278         -0.062147        0.760834       0.750528     0.0     1.0  \n",
       "40138          0.126346        0.803004       0.691211     0.0     1.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning Using Grid Search Cross Validation\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "params[\"cv\"] = {}\n",
    "params[\"cv\"][\"name\"] = \"random forest\"\n",
    "\n",
    "params[\"cv\"][\"model_params\"] =  {\n",
    "\t\t\t\t\t\t\t\"n_estimators\": [100, 125, 150, 175, 200],\n",
    "\t\t\t\t\t\t\t\"max_depth\": [4],\n",
    "\t\t\t\t\t\t\t\"random_state\": [2],\n",
    "\t\t\t\t\t\t\t\"min_samples_split\": [5]\n",
    "\t\t\t\t\t\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  50 | elapsed:   11.1s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 05e41391e1044ceabef83624f0cf7dc6\n",
      "MLflow Run ID: 05d71399a6e8422ea283d5f8091c2e47\n",
      "MLflow Run ID: 2846905f7e1f4d8f96c6c3479e184b6a\n",
      "MLflow Run ID: 0bae87313f764a65b3a174e1d71c9035\n",
      "MLflow Run ID: 4bbe36b5981940f084ceff3d9d9405ee\n"
     ]
    }
   ],
   "source": [
    "model_rf, accuracy_rf, params_rf = prep_ml(params, abt, clf=\"random forest\", model_params=params['model']['model_params'], cv=True, gridsearch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "params[\"cv\"] = {}\n",
    "params[\"cv\"][\"name\"] = \"XGBoost\"\n",
    "\n",
    "params[\"cv\"][\"model_params\"] = {\n",
    "\t\t\t\t\t\t\t\"clf__n_estimators\": [50, 100, 150, 200],\n",
    "\t\t\t\t\t\t\t\"clf__learning_rate\": [0.01],\n",
    "\t\t\t\t\t\t\t\"clf__max_depth\": [3],\n",
    "\t\t\t\t\t\t\t\"clf__colsample_bytree\": [0.5],\n",
    "\t\t\t\t\t\t\t\"clf__gamma\": [0.2],\n",
    "                            \"min_child_weight\": [1, 3, 5, 7]\n",
    "\t\t\t\t\t\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  2.1min finished\n",
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { clf__colsample_bytree, clf__gamma, clf__learning_rate, clf__max_depth, clf__n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:28:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost\n",
      "[19:28:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { clf__colsample_bytree, clf__gamma, clf__learning_rate, clf__max_depth, clf__n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:28:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "MLflow Run ID: 5d5574d3b5304d66ab4bcc7714158db9\n",
      "MLflow Run ID: 8d6a22ac792342bbbdc83a132636f4db\n",
      "MLflow Run ID: 71475b69914f44fa81d04d0f09bc8ef2\n",
      "MLflow Run ID: 8959f98acf8e4f15b4487fbcfc501a8a\n",
      "MLflow Run ID: a038321077d44b139907095754200984\n",
      "MLflow Run ID: 775c42a611e5457381c4832bad843210\n",
      "MLflow Run ID: 9e43ecf5855d48c19638ad30515d0bc0\n",
      "MLflow Run ID: 5fef8de1d0e4448caae24336b561d923\n",
      "MLflow Run ID: 7cd89469d6a94d27bb15e265b27a3568\n",
      "MLflow Run ID: c32f3d437a6d4768992c30e3604ad6a3\n",
      "MLflow Run ID: 7079b9c86c124c34b35170d8266f2b33\n",
      "MLflow Run ID: 2bc6042f785a44a2a98aa9bfe6fdb237\n",
      "MLflow Run ID: bd42468ef43e416d8ae97b24529b5ded\n",
      "MLflow Run ID: 9719ca53d81a45cb9bca6913b9f56708\n",
      "MLflow Run ID: 8a27ab579bcd429eb8ad6c3ca78d0c0d\n",
      "MLflow Run ID: f1bc071bc943499184dffe2729ecba92\n"
     ]
    }
   ],
   "source": [
    "model_xgb, accuracy_xgb, params_xgb = prep_ml(params, abt, clf=\"XGBoost\", model_params=params['model']['model_params'], cv=True, gridsearch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on the test dataset\n",
    "\n",
    "To do that, we shall first create the base table for the test dataset by parsing and indexing the test set in the same way as was done for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      " 19%|█▉        | 6/31 [00:00<00:00, 49.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing clinical metadata.. please wait..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 42.24it/s]\n",
      "100%|██████████| 245/245 [00:00<00:00, 16311.70it/s]\n",
      "100%|██████████| 233/233 [00:00<00:00, 1572.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start converting descriptors to vectors\n",
      "Took 0.000303 minutes to vectorize the dataset\n",
      "Start converting descriptors to vectors\n",
      "Took 0.002521 minutes to vectorize the dataset\n"
     ]
    }
   ],
   "source": [
    "tdp1 = tdp.TCGA_data_processor(clinical_data_test_dir,True)\n",
    "test_data = tdp1.get_parsed_data()\n",
    "test_abt =cde_data_modellers.create_abt(test_data)\n",
    "\n",
    "test_abt.fillna(0, inplace = True)\n",
    "\n",
    "index_cols = ['headers','public_id']\n",
    "header_col = index_cols[0]\n",
    "id_col = index_cols [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the final model, logging experiment on MLFlow and export final params file\n",
    "\n",
    "Note that I have created a model.predict_and_convert_to_json function which returns the prediction in the following format: <br>\n",
    "{\n",
    "clinical parameter1: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    "clinical parameter2: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    ".....\n",
    "clinical parametern: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Compare ML Models' does not exist. Creating a new experiment\n",
      "MLflow Run ID: e1cb25f21b6e44c1a7e8cd43103a2012\n",
      "MLflow Run ID: 4fe5dc4f1ac2473195be682434adaa68\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_xgb = make_predictions(model_xgb, accuracy_xgb,\"XGBoost\", params_xgb, test_abt.iloc[:, :-1], 20, index_cols, header_col, id_col, clinical_data_test_dir, json_output=False, accuracy_and_mlflow_output=True)\n",
    "test_accuracy_rf = make_predictions(model_rf, accuracy_xgb,\"random forest\", params_rf, test_abt.iloc[:, :-1], 20, index_cols, header_col, id_col, clinical_data_test_dir, json_output=False, accuracy_and_mlflow_output=True)\n",
    "\n",
    "# pick a model and update params dictionary\n",
    "if test_accuracy_xgb > test_accuracy_rf:\n",
    "    accuracy = accuracy_xgb\n",
    "    test_accuracy = test_accuracy_xgb\n",
    "    params['model']['name'] = \"XGBoost\"\n",
    "    params['model']['model_params'] = params_xgb\n",
    "else:\n",
    "    accuracy = accuracy_rf\n",
    "    test_accuracy = test_accuracy_rf\n",
    "    params['model']['name'] = \"random forest\"\n",
    "    params['model']['model_params'] = params_rf\n",
    "\n",
    "# output of final_params.json\n",
    "\n",
    "with open(\"params_supervised_final.json\",\"w\") as outfile:\n",
    "    json.dump(params,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain final model to get final output\n",
    "## 1. Reload test and training datasets\n",
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data_files_dir = 'Hackathon_Data/tcga_training_data/'\n",
    "\n",
    "clinical_data_test_dir = 'Hackathon_Data/tcga_test_data/'\n",
    "\n",
    "parameter_file = 'params_supervised_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model parameters\n",
    "params = {}\n",
    "\n",
    "with open(parameter_file,'r') as file:\n",
    "    params = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reload fast text if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CDE database... please wait\n",
      "Took 0.001875  minutes to load CDE database..\n"
     ]
    }
   ],
   "source": [
    "cde_data_modellers = cdm.CDE_data_modeller(cde_database_file, params)\n",
    "cde_data_modellers.load_model_and_cde_indexes(model_dir+'window_'+str(params['fasttext']['window'])+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load, parse and create abt for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "  3%|▎         | 6/200 [00:00<00:03, 51.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing clinical metadata.. please wait..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 48.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tdpr = tdp.TCGA_data_processor(clinical_data_files_dir,True )\n",
    "tcga_data = tdpr.get_parsed_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 712/712 [00:00<00:00, 14570.73it/s]\n",
      "  1%|▏         | 10/701 [00:00<00:07, 89.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start converting descriptors to vectors\n",
      "Took 0.000892 minutes to vectorize the dataset\n",
      "Start converting descriptors to vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 701/701 [00:00<00:00, 767.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.015304 minutes to vectorize the dataset\n"
     ]
    }
   ],
   "source": [
    "abt = cde_data_modellers.create_abt(tcga_data, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[12:04:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { clf__colsample_bytree, clf__gamma, clf__learning_rate, clf__max_depth, clf__n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:04:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9633251833740831, 'balanced_accuracy': 0.9241642958748222, 'f1': 0.8972602739726028, 'precision': 0.9357142857142857, 'recall': 0.8618421052631579, 'auroc': 0.9847182709024815}\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = prep_ml(params, abt, clf=params['model']['name'], model_params=params['model']['model_params'], cv=False, gridsearch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load, parse and create abt for test data\n",
    "### Disclaimer: might need to debug this section, didn't have the chance to test as csv is different in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 6/6 [00:00<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing clinical metadata.. please wait..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:00<00:00, 15035.98it/s]\n",
      "100%|██████████| 225/225 [00:00<00:00, 2155.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start converting descriptors to vectors\n",
      "Took 0.000301 minutes to vectorize the dataset\n",
      "Start converting descriptors to vectors\n",
      "Took 0.001786 minutes to vectorize the dataset\n"
     ]
    }
   ],
   "source": [
    "tdp1 = tdp.TCGA_data_processor(clinical_data_test_dir,False)\n",
    "test_data = tdp1.get_parsed_data()\n",
    "test_abt =cde_data_modellers.create_abt(test_data)\n",
    "\n",
    "test_abt.fillna(0, inplace = True)\n",
    "\n",
    "index_cols = ['headers','public_id']\n",
    "header_col = index_cols[0]\n",
    "id_col = index_cols [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save final prediction\n",
    "### Disclaimer: might need to debug this section, didn't have the chance to test as csv is different in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \n",
    "make_predictions(model, accuracy,params['model']['name'], params['model']['model_params'], test_abt.iloc[:, :-1], 20, index_cols, header_col, id_col, clinical_data_test_dir, json_output=True,accuracy_and_mlflow_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHIVE\n",
    "\n",
    "## Calculate accuracy of prediction for the test dataset\n",
    "\n",
    "Modified test dataset used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_gs = {}\n",
    "# #with open(test_gold_standard, 'rb') as file:\n",
    "# #    test_gs = json.load(file)\n",
    "\n",
    "\n",
    "# all_files = glob.glob(clinical_data_test_dir + \"*.txt\")\n",
    "\n",
    "# test_gs_dict ={}\n",
    "\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col = 0, header = 0, sep='\\t')\n",
    "    \n",
    "#     #keeping only rows that has CDE number\n",
    "#     df = df[df.iloc[:, 0].str.contains(\"CDE\")]\n",
    "#     df.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "#     #striping 'CDE_ID' from string, leaving only the ID numbers\n",
    "#     test_gs_dict.update(df.iloc[0,:].apply(lambda x: x.replace('CDE_ID:','')).to_dict())\n",
    "\n",
    "# # remove empty header if any\n",
    "# #test_gs_dict = {key:val for key, val in test_dict.items() if val != ''}\n",
    "\n",
    "# test_accuracy = ac.calculate_accuracy(test_gs_dict,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log model parameters etc. using mlflow\n",
    "\n",
    "This will ensure reproducibility of results and will keep track of all models and results during the model development and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment('Compare ML Models')\n",
    "# with mlflow.start_run():\n",
    "#     # print out current run_uuid\n",
    "#     run_uuid = mlflow.active_run().info.run_uuid\n",
    "#     print(\"MLflow Run ID: %s\" % run_uuid)\n",
    "    \n",
    "#     # log parameters\n",
    "#     mlflow.log_param(\"window_size\", params[\"fasttext\"][\"window\"])\n",
    "#     mlflow.log_param(\"min_count\", params[\"fasttext\"][\"min_count\"])\n",
    "#     mlflow.log_param(\"epochs\", params[\"fasttext\"][\"epochs\"])\n",
    "#     mlflow.log_param(\"vector_size\", params[\"fasttext\"][\"vector_size\"])\n",
    "    \n",
    "    \n",
    "#     mlflow.log_param(\"features_diference_types\", params[\"features\"][\"differences\"][\"type\"])\n",
    "#     mlflow.log_param(\"features_metrics\", params[\"features\"][\"metrics\"][\"metric\"])\n",
    "#     mlflow.log_param(\"features_metrics_sim_type\", params[\"features\"][\"metrics\"][\"sim_type\"])\n",
    "#     mlflow.log_param(\"features_metrics_scaling\", params[\"features\"][\"metrics\"][\"scaling\"])\n",
    "#     mlflow.log_param(\"features_sampling_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
    "    \n",
    "#     mlflow.log_param(\"features_samplinf_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
    "    \n",
    "#     mlflow.log_param(\"model_type\", params['model'][\"name\"])\n",
    "    \n",
    "#     for k in params['model']['model_params'].keys():\n",
    "#         mlflow.log_param(\"model_params_\"+k, params['model'][\"model_params\"][k])\n",
    "    \n",
    "#     # log metrics\n",
    "#     #CHECK THIS SECTION\n",
    "#     mlflow.log_metric(\"test_accuracy\",test_accuracy)\n",
    "#     for k in accuracy['mean'].keys():\n",
    "#         if 'confusion' not in k:\n",
    "#             mlflow.log_metric(\"val_accuracy_\"+k,accuracy['mean'][k])\n",
    "#             mlflow.log_metric(\"val_accuracy_\"+k,accuracy['std'][k])\n",
    "    \n",
    "#     #mlflow.sklearn.logmodel()\n",
    "#     with open('models/'+run_uuid+'.pkl','wb') as file:\n",
    "#         pickle.dump(model, file)\n",
    "    \n",
    "#     mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 'mlflow ui' to compare and analyze various model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haokunli/Desktop/hack/friendsofpatrickstarr/newenv/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-01-29 19:30:47 +0000] [50207] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-01-29 19:30:47 +0000] [50207] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2021-01-29 19:30:47 +0000] [50207] [ERROR] Retrying in 1 second.\n",
      "[2021-01-29 19:30:48 +0000] [50207] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2021-01-29 19:30:48 +0000] [50207] [ERROR] Retrying in 1 second.\n",
      "[2021-01-29 19:30:49 +0000] [50207] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2021-01-29 19:30:49 +0000] [50207] [ERROR] Retrying in 1 second.\n",
      "[2021-01-29 19:30:50 +0000] [50207] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2021-01-29 19:30:50 +0000] [50207] [ERROR] Retrying in 1 second.\n",
      "[2021-01-29 19:30:51 +0000] [50207] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2021-01-29 19:30:51 +0000] [50207] [ERROR] Retrying in 1 second.\n",
      "[2021-01-29 19:30:52 +0000] [50207] [ERROR] Can't connect to ('127.0.0.1', 5000)\n",
      "Running the mlflow server failed. Please see the logs above for details.\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view mlflow ui go to http://localhost:5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
