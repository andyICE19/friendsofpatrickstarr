{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated metadata matching\n",
    "\n",
    "Life sciences (LS)/ Clinical research institutes (academic research institutes, pharma companies, hospitals, clinics etc.) across the world are producing large volumes of data from patients. This can range from clinical information such as diagnostic/prognostic data, omic data such as genetic/proteomic/epigenetic screens, pathological data such as MRI scans etc. One of the main objectives in LS research (both academic and industrial) is to gain actionable insights from these data sets, that goes beyond the diagnosis/prognosis of a (group of) patient(s) and provides a deeper understanding of the diseases, as well as shine lights on new therapeutic options. It is becoming apparent, that to gain actionable insights from LS data sets, we need data from a large number of patients. This is achievable, if we could merge datasets from various institutes, which is turning out to be hugely challenging task, simply because different institutes use different standards, units, nomenclature etc. to store data. <br><br>\n",
    "\n",
    "For instance, patient's age is a common clinical parameter recorded by almost all organisations. One institute can name the variable that records patients’ age as 'patient age', another can name the same variable as 'age', others can name it as 'age at diagnosis', 'days since birth', 'years since birth' etc. The values can also be in days, months, years etc. Therefore, to combine data from many institutes (and sometimes within same institutes), it's essential to understand that all of the above variables are recording the same thing, i.e. patient's age, also we need to make sure that the units (days, months, years) of measuring age are homogenised at the time of integration. <br><br>\n",
    "\n",
    "To assist in the above process the National Cancer Institute (NCI) created the concept of CDE (common data element). See https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search for more details. A big data dump of about 69000 CDE elements are provided in 'cde_database\\full_database' folder in XML format, if you want to further explore. They provide a standard format of representing Life Science's data. This gives us standard variable name, the permissible values , units etc. for each of these clinical parameters. Some research organizations are following this standard, but vast majority aren't. Additionally, there is huge amount of data produced until now which are not standardized using CDEs. <br><br>\n",
    "\n",
    "To be able to integrate data from various institutes, we need to be able to match the variable names in the clinical datasets to the corresponding CDE elements. Currently, there is a drive for developing ML/AI algorithms to achieve this.<br><br>\n",
    "\n",
    "\n",
    "The code below is an initial attempt in this direction. In summary, it tries to match the variables names (generally the column headers in a clinical data file) and values (the column values) of the clinical parameters in a dataset, to the long variable names, and permissible values of the CDE elements. The objective is to find the CDE elements that closely match the each clinical parameter name (i.e. the column header). To do the the following steps are performed: <br><br>\n",
    "\n",
    "1. Converted selected aspects (e.g. long_name, permissible_values etc.) of all CDE elements into numerical vectors using a word embedding model which itself was trained on these data.\n",
    "\n",
    "2. Coverted the clinical parameter names (headers) and values into numerical vector using the same word embedding model as above.\n",
    "\n",
    "3. The vectors from the clinical data can be matched to CDE vectors in a few different ways: <br>\n",
    "   (a) One way is to use unsupervised learning, fit a Nearest Neighbour model to the CDE vectors, and look for the nearest neighbors of each clinical parameter using this model.\n",
    "   (b) Another way is to use supervised learning: Create feature vectors for all possible pairs of clinical parameters and CDE elements, consider the true pairs as positive class (target =1) and the remaining pairs as negative class (target = 0). Train classifiers to this data and use the classifier to evaluate new clinical parameters.\n",
    "   \n",
    " \n",
    "See more details below.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install custom benchmark solutions libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install cde_modelling_tools/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\sher.lynn.wong\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\nscipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import mlflow\n",
    "from cde_modelling.modelling import CDE_data_modeller as cdm\n",
    "from cde_modelling.parsing import TCGA_data_parser as tdp\n",
    "from cde_modelling.utils import Accuracy_calculations as ac\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data_files_dir = 'tcga_training_data/'\n",
    "\n",
    "clinical_data_test_dir = 'tcga_test_data'\n",
    "\n",
    "cde_database_file = 'combined_small_dataset'\n",
    "#cde_database_file = 'xml_cde_2020102112645_1.xml'\n",
    "#cde_database_file = '1-and-10'\n",
    "#cde_database_file = 'cde_database_full' #not exactly full, it's 1 and 10-15\n",
    "parameter_file = 'params_supervised.json'\n",
    "\n",
    "model_dir = 'models/'\n",
    "\n",
    "test_gold_standard = 'gold_standard/test_gs.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model parameters\n",
    "params = {}\n",
    "\n",
    "with open(parameter_file,'r') as file:\n",
    "    params = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fasttext model for the CDE database and index the individual CDE elements in the database \n",
    "\n",
    "Fasttext is a word embedding algorithm developed by FaceBook. Given a corpus, it creates a model that tries to predict if a pair of words appear in the same context. The model first converts the words to a numeric vector which are used as features for the above prediction. We are interested in the feature generation part, i.e. the part which converts words to numeric vectors. For more information on the FastText model see https://radimrehurek.com/gensim/models/fasttext.html.\n",
    "\n",
    "### FastText model training: \n",
    "\n",
    "To train a FastText model we first extracted the long_name and permissible_values of each CDE elements. These were then parseed and cleaned (lower cased, alphanumeric character only, splitted into bag of words). The preprocessed long names and permissible values of all CDE element was considered as the training corpus for the FastText model. The corpus was then used to train A FastText model. The parameters for the model are in the above json file. The trained model is then used to index the CDE elements (i.e. create numeric vectors representing each CDE). We created two sets of vectors for CDE elements, one for the long_names and the other for permissible values. We alo extracted the data_type information for each CDE elements. Below is an example. Let's assume that the following is a (oversimplified) CDE element .\n",
    "CDE_element: \n",
    "{\n",
    "'public_id': 1234\n",
    "'long_name': 'received radiotherapy'\n",
    "..............\n",
    "'permissible_values': ['yes','no']\n",
    "}.\n",
    "\n",
    "To index the above CDE, we performed the following:\n",
    "\n",
    "1. Vectorized the long_name entry (i.e. 'received radiotherapy') using the FastText model. To do that, we vectorized each word (i.e. 'received' and 'radiotherapy') of the long name entry separately. The vectors were then normalized by their L2 norms and averaged. Say for example, the long_name vector is [0.1, 0.345]. \n",
    "\n",
    "2. Vectorized the 'permissible_values' entry (i.e. 'yes', 'no') using the FastText model. To do that, we vectorized each word (i.e. 'received' and 'radiotherapy') in the permissible_values entry separately. The vectors were then normalized by their L2 norms and averaged.Say for example, the permissible vector is [0.981, 0.233]. \n",
    "\n",
    "3. We identified whether the permissible values are string or numbers. Note, that for the benchmark solution, we kept this simple. But for the hackathon, the participants can conder more grannular data type for example, string, binary, float, int long etc.\n",
    "\n",
    "\n",
    "Combination of the above is used to numerically represent (index) each CDE. The class CDE_data_modeller, in package cde_modelling_tools does the above. Pparticipants should explore using other entries in the CDE data fields to improve their chances of finding a match.\n",
    "\n",
    "The CDE_data_modeller class not only creates the word embedding models and index (vectorize) the CDE data elements, it can also save and load pretrained models and indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_data_modellers = cdm.CDE_data_modeller('cde_database/'+cde_database_file+'.json', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "cde_data_modellers.create_model_and_cde_indexes()\n",
    "cde_data_modellers.save_model_and_indexes(model_dir+'fasttext/'+cde_database_file+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained FastText model and saved indexes for CDE elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading CDE database... please wait\nTook 0.001473  minutes to load CDE database..\n"
     ]
    }
   ],
   "source": [
    "cde_data_modellers = cdm.CDE_data_modeller('cde_database/'+cde_database_file+'.json', params)\n",
    "cde_data_modellers.load_model_and_cde_indexes(model_dir+'fasttext/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and parse training data\n",
    "\n",
    "The training data are a set of clinical data files which records cinical information of patients, e.g. gender, age, disease_type, disease_sub_type, treatment received etc. It's in table format, where the rows represent patients and the columns represent colinical parameters. In case of the training data, the CDE data element corrsponding to each clinical parameter is provided. This information can be used to train machine learning algorithms to predict CDE elements for new clinical parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s] Processing clinical metadata.. please wait..\n",
      "100%|██████████| 200/200 [00:06<00:00, 31.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tdpr = tdp.TCGA_data_processor(clinical_data_files_dir,True )\n",
    "tcga_data = tdpr.get_parsed_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser returns three types of information for each clinical parameter.\n",
    "\n",
    "1. The name of the parameter (e.g. age, gender, etc.)\n",
    "2. List of values for each parameters (except id columns, continuous variabales etc.)\n",
    "3. Data type of the values. For instance, data type of 'age' is 'number', data type of gender = 'string'. \n",
    "4. A dictionary containing clinical parameters and it's corresponding \n",
    "\n",
    "See the parsed data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['headers', 'values', 'value_type', 'gold_standard'])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tcga_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data['gold_standard']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base tables for model training\n",
    "\n",
    "To create base tables I performed the following:\n",
    "\n",
    "1. Indexed (vectorized using the FastText model) the headers (clinical parameter names) and values of each clinical parameters parsed in the previous step.\n",
    "2. for each possible pair of clinical_parameter and CDE elements we calculate the following features <br>\n",
    "    (a) Difference between the embedding vectors of the CDE long_name and the clinical parameter name. <br>\n",
    "    (b) Difference between the embedding vectors of the CDE permissible values and the values associated with the clinical parameters in the training dataset. <br>\n",
    "    (c) A similarity measure (cosine similarity, correlation etc.) betwween the CDE long_name and clinical parameter name vectors <br>\n",
    "    (d) A similarity measure (cosine similarity, correlation etc.) betwween the CDE permissible_vaue and observed clinical parameter value vectors <br>\n",
    "    (e) Similaritied between the data type of the permissible and observed values of the CDE and the observed clinical oaraneters respectively <br>\n",
    "\n",
    "3. Note that, in the base table one data point is represented by a pair (clinical parameter and a CDE ). For example: If there are 800 cinical parameters in the training data and 5000 CDE elements in the CDE dataset, the the base table will have 500*8000 = 4million entries. Each entry will have the above features. The 'target' variable is defined as follows: <br>\n",
    "\n",
    "$\n",
    "target = 1, \\text{if the CDE element is manually matched to the clinical parameter} \\\\\n",
    "target = 0, \\text{otherwise}\n",
    "$\n",
    "\n",
    "In the above example, there are 800 clinical parameters, and if only 1 CDE elements is matched to each clinical parameter, the target variable can be equals to 1 in only 800 out of 4 million cases. Therefore the base table is extremely imbalanced. To counter this we need to undersample (or oversample) the abt. The create_abt function in CDE_data_modeller allows undersampling. The ratio of undersampling (number of cases target = 0 / number of cases target =1 ) can be adjusted using the params dictionary. The defalut value is 5 which means in the undersampled base tables, 16.67 % of cases have target =1 and 83.33% of cases have target = 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_data['headers'] # tcga_data['values'] / tcga_data['value_type'] / tcga_data['gold_standard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['features']['differences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 712/712 [00:00<00:00, 11607.10it/s]\n",
      "  0%|          | 0/701 [00:00<?, ?it/s]Start converting descriptors to vectors\n",
      "Took 0.001405 minutes to vectorize the dataset\n",
      "Start converting descriptors to vectors\n",
      "100%|██████████| 701/701 [00:01<00:00, 666.44it/s]\n",
      "Took 0.017672 minutes to vectorize the dataset\n"
     ]
    }
   ],
   "source": [
    "abt = cde_data_modellers.create_abt(tcga_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       feature_vec_0_x  feature_vec_1_x  feature_vec_2_x  feature_vec_3_x  \\\n",
       "20019        -0.074879         0.044559        -0.151783        -0.005449   \n",
       "25204         0.003034         0.001879         0.029784         0.058840   \n",
       "30658        -0.047950         0.021755         0.013564        -0.008411   \n",
       "35381        -0.017392        -0.023105        -0.014109        -0.058086   \n",
       "35382        -0.017392        -0.023105        -0.014109        -0.058086   \n",
       "\n",
       "       feature_vec_4_x  feature_vec_5_x  feature_vec_6_x  feature_vec_7_x  \\\n",
       "20019        -0.010106        -0.151557        -0.010543        -0.081013   \n",
       "25204         0.013281         0.039684         0.030796         0.025707   \n",
       "30658         0.107960        -0.055107        -0.035221        -0.002115   \n",
       "35381        -0.075214        -0.057065         0.057395        -0.120960   \n",
       "35382        -0.075214        -0.057065         0.057395        -0.120960   \n",
       "\n",
       "       feature_vec_8_x  feature_vec_9_x  ...  feature_vec_44_y  \\\n",
       "20019        -0.057839         0.023783  ...         -0.020757   \n",
       "25204         0.029569        -0.026366  ...         -0.037163   \n",
       "30658        -0.013555         0.088978  ...         -0.016799   \n",
       "35381         0.008653        -0.081426  ...         -0.035563   \n",
       "35382         0.008653        -0.081426  ...         -0.035563   \n",
       "\n",
       "       feature_vec_45_y  feature_vec_46_y  feature_vec_47_y  feature_vec_48_y  \\\n",
       "20019         -0.052062          0.051249          0.013284         -0.074078   \n",
       "25204         -0.024893          0.044011         -0.020780         -0.072418   \n",
       "30658          0.021785          0.007958          0.035195          0.035951   \n",
       "35381         -0.058119          0.100232         -0.017497          0.045034   \n",
       "35382         -0.058119          0.100232         -0.017497          0.045034   \n",
       "\n",
       "       feature_vec_49_y  header_metrics  value_metrics  metric  target  \n",
       "20019         -0.016504        0.750432       0.728281     0.0     1.0  \n",
       "25204         -0.000053        0.898956       0.785171     0.0     1.0  \n",
       "30658          0.008849        0.784019       0.745251     0.0     1.0  \n",
       "35381         -0.001179        0.793131       0.716037     0.0     1.0  \n",
       "35382         -0.001179        0.793131       0.716037     1.0     1.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_vec_0_x</th>\n      <th>feature_vec_1_x</th>\n      <th>feature_vec_2_x</th>\n      <th>feature_vec_3_x</th>\n      <th>feature_vec_4_x</th>\n      <th>feature_vec_5_x</th>\n      <th>feature_vec_6_x</th>\n      <th>feature_vec_7_x</th>\n      <th>feature_vec_8_x</th>\n      <th>feature_vec_9_x</th>\n      <th>...</th>\n      <th>feature_vec_44_y</th>\n      <th>feature_vec_45_y</th>\n      <th>feature_vec_46_y</th>\n      <th>feature_vec_47_y</th>\n      <th>feature_vec_48_y</th>\n      <th>feature_vec_49_y</th>\n      <th>header_metrics</th>\n      <th>value_metrics</th>\n      <th>metric</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20019</th>\n      <td>-0.074879</td>\n      <td>0.044559</td>\n      <td>-0.151783</td>\n      <td>-0.005449</td>\n      <td>-0.010106</td>\n      <td>-0.151557</td>\n      <td>-0.010543</td>\n      <td>-0.081013</td>\n      <td>-0.057839</td>\n      <td>0.023783</td>\n      <td>...</td>\n      <td>-0.020757</td>\n      <td>-0.052062</td>\n      <td>0.051249</td>\n      <td>0.013284</td>\n      <td>-0.074078</td>\n      <td>-0.016504</td>\n      <td>0.750432</td>\n      <td>0.728281</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25204</th>\n      <td>0.003034</td>\n      <td>0.001879</td>\n      <td>0.029784</td>\n      <td>0.058840</td>\n      <td>0.013281</td>\n      <td>0.039684</td>\n      <td>0.030796</td>\n      <td>0.025707</td>\n      <td>0.029569</td>\n      <td>-0.026366</td>\n      <td>...</td>\n      <td>-0.037163</td>\n      <td>-0.024893</td>\n      <td>0.044011</td>\n      <td>-0.020780</td>\n      <td>-0.072418</td>\n      <td>-0.000053</td>\n      <td>0.898956</td>\n      <td>0.785171</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>30658</th>\n      <td>-0.047950</td>\n      <td>0.021755</td>\n      <td>0.013564</td>\n      <td>-0.008411</td>\n      <td>0.107960</td>\n      <td>-0.055107</td>\n      <td>-0.035221</td>\n      <td>-0.002115</td>\n      <td>-0.013555</td>\n      <td>0.088978</td>\n      <td>...</td>\n      <td>-0.016799</td>\n      <td>0.021785</td>\n      <td>0.007958</td>\n      <td>0.035195</td>\n      <td>0.035951</td>\n      <td>0.008849</td>\n      <td>0.784019</td>\n      <td>0.745251</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>35381</th>\n      <td>-0.017392</td>\n      <td>-0.023105</td>\n      <td>-0.014109</td>\n      <td>-0.058086</td>\n      <td>-0.075214</td>\n      <td>-0.057065</td>\n      <td>0.057395</td>\n      <td>-0.120960</td>\n      <td>0.008653</td>\n      <td>-0.081426</td>\n      <td>...</td>\n      <td>-0.035563</td>\n      <td>-0.058119</td>\n      <td>0.100232</td>\n      <td>-0.017497</td>\n      <td>0.045034</td>\n      <td>-0.001179</td>\n      <td>0.793131</td>\n      <td>0.716037</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>35382</th>\n      <td>-0.017392</td>\n      <td>-0.023105</td>\n      <td>-0.014109</td>\n      <td>-0.058086</td>\n      <td>-0.075214</td>\n      <td>-0.057065</td>\n      <td>0.057395</td>\n      <td>-0.120960</td>\n      <td>0.008653</td>\n      <td>-0.081426</td>\n      <td>...</td>\n      <td>-0.035563</td>\n      <td>-0.058119</td>\n      <td>0.100232</td>\n      <td>-0.017497</td>\n      <td>0.045034</td>\n      <td>-0.001179</td>\n      <td>0.793131</td>\n      <td>0.716037</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 106 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a machine learning model using the abt created above\n",
    "I created a separate class called create_model where a number of supervised and unsupervised learning algorithms are implemented (from sklearn library). The type and parameters of the model can be passed using the params dictionary. \n",
    "\n",
    "!!! Warning: Currently the the two datatype columns in the abt (data_type_string, data_type_number) are complementary and hence redundant. Only one should be used for modelling. This needs to be corrected in the future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create feature vectors\n",
    "features = [c for c in abt.columns if ('feature' in c) or ('metric' in c)]\n",
    "\n",
    "# create design matrix and target data\n",
    "X = abt[features]\n",
    "y= abt['target']\n",
    "\n",
    "# create training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "# import the Model class\n",
    "from cde_modelling.modelling.create_models import Model\n",
    "\n",
    "# create model\n",
    "model = Model(params)\n",
    "\n",
    "#fit the model\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# calculate the accuracy of the model on the validation data\n",
    "accuracy = model.accuracy(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on the test dataset\n",
    "\n",
    "To do that, we shall first create the base table for the test dataset by parsing and indexing the test set in the same way as was done for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdp1 = tdp.TCGA_data_processor(clinical_data_test_dir,False )\n",
    "test_data = tdp1.get_parsed_data()\n",
    "test_abt =cde_data_modellers.create_abt(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for the test dataset using the trained model\n",
    "\n",
    "Note that I have created a model.predict_and_convert_to_json function which returns the prediction in the following format: <br>\n",
    "{\n",
    "clinical parameter1: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    "clinical parameter2: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    ".....\n",
    "clinical parametern: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abt.fillna(0, inplace = True)\n",
    "\n",
    "index_cols = ['headers','public_id']\n",
    "header_col = index_cols[0]\n",
    "id_col = index_cols [1]\n",
    "\n",
    "results = model.predict_and_convert_to_json(test_abt,20, index_cols, header_col, id_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy of prediction for the test dataset\n",
    "\n",
    "Note that the participants won't have access to the gold standard data, therefore won't be able to perform the following step. However, participants can divide the training data in to train, test, validation sets and perform the following on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gs = {}\n",
    "with open(test_gold_standard, 'rb') as file:\n",
    "    test_gs = json.load(file)\n",
    "test_accuracy = ac.calculate_accuracy(test_gs,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log model parameters etc. using mlflow\n",
    "\n",
    "This will ensure reproducibility of results and will keep track of all models and results during the model development and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLflow Run ID: e3782b28cbbb4d7f969a22e97e4913b9\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # print out current run_uuid\n",
    "    run_uuid = mlflow.active_run().info.run_uuid\n",
    "    print(\"MLflow Run ID: %s\" % run_uuid)\n",
    "    \n",
    "    # log parameters\n",
    "    mlflow.log_param(\"window_size\", params[\"fasttext\"][\"window\"])\n",
    "    mlflow.log_param(\"min_count\", params[\"fasttext\"][\"min_count\"])\n",
    "    mlflow.log_param(\"epochs\", params[\"fasttext\"][\"epochs\"])\n",
    "    mlflow.log_param(\"vector_size\", params[\"fasttext\"][\"vector_size\"])\n",
    "    \n",
    "    \n",
    "    mlflow.log_param(\"features_diference_types\", params[\"features\"][\"differences\"][\"type\"])\n",
    "    mlflow.log_param(\"features_metrics\", params[\"features\"][\"metrics\"][\"metric\"])\n",
    "    mlflow.log_param(\"features_metrics_sim_type\", params[\"features\"][\"metrics\"][\"sim_type\"])\n",
    "    mlflow.log_param(\"features_metrics_scaling\", params[\"features\"][\"metrics\"][\"scaling\"])\n",
    "    mlflow.log_param(\"features_sampling_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
    "    \n",
    "    mlflow.log_param(\"features_samplinf_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", params['model'][\"name\"])\n",
    "    \n",
    "    for k in params['model']['model_params'].keys():\n",
    "        mlflow.log_param(\"model_params_\"+k, params['model'][\"model_params\"][k])\n",
    "    \n",
    "    # log metrics\n",
    "        \n",
    "    #mlflow.log_metric(\"test_accuracy\",test_accuracy)\n",
    "    for k in accuracy.keys():\n",
    "        if 'confusion' not in k:\n",
    "            mlflow.log_metric(\"val_accuracy_\"+k,accuracy[k])\n",
    "    \n",
    "    #mlflow.sklearn.logmodel()\n",
    "    with open('models/'+run_uuid+'.pkl','wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 'mlflow ui' to compare and analyze various model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mlflow_ui.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view mlflow ui go to http://localhost:5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}